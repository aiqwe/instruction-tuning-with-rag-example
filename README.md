# ëª©ì  
ì´ ì½”ë“œëŠ” [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë°ì´í„° ìƒì„±, í•™ìŠµ, í‰ê°€ê¹Œì§€ ë°‘ë°”ë‹¥ë¶€í„° Instruction Tuningì„ ì‹¤ìŠµí•´ë³´ëŠ” ì½”ë“œì…ë‹ˆë‹¤.  
ì°¸ì¡° ë…¼ë¬¸ê³¼ ê¹ƒí—ˆë¸Œë“¤ì€ ë§í¬ê°€ ê±¸ë ¤ìˆìœ¼ë‹ˆ, ë˜ë„ë¡ ì›ë³¸ ë¬¸ì„œë¥¼ ê°™ì´ ë³´ë©´ì„œ í•™ìŠµí•´ë³´ë©´ í° ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. 

# ëª©ì°¨
[ê°œìš”](#ê°œìš”)  
[ì˜ˆì œ ëª¨ë¸](#ì˜ˆì œ-ëª¨ë¸)  
[ì½”ë“œ íŠ¸ë¦¬](#ì½”ë“œ-íŠ¸ë¦¬)  
[í›ˆë ¨ ìŠ¤í™](#í›ˆë ¨-ìŠ¤í™)  
[Colab ê°€ì´ë“œ](#Colab-ê°€ì´ë“œ)  
[ì°¸ì¡°](#ì°¸ì¡°)  

# ê°œìš”
ì´ ì˜ˆì œ ì½”ë“œ í¬ê²Œ 3ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
1. OpenAIì˜ GPT APIë¥¼ í†µí•´ [ë°ì´í„°ë¥¼ ìƒì„±](data/instruction.jsonl)í•©ë‹ˆë‹¤. 
2. ìƒì„±í•œ ë°ì´í„°ë¡œ [gemma-2b-it](https://huggingface.co/google/gemma-2b-it) ëª¨ë¸ì„ Fine-Tuningí•©ë‹ˆë‹¤.
3. í•™ìŠµí•œ ëª¨ë¸ì„ [LLMìœ¼ë¡œ í‰ê°€](https://arxiv.org/pdf/2306.05685)í•©ë‹ˆë‹¤.

# ì˜ˆì œ ëª¨ë¸
ì˜ˆì œ ëª¨ë¸ì€ [https://huggingface.co/aiqwe/gemma-2b-it-example-v1](https://huggingface.co/aiqwe/gemma-2b-it-example-v1)ë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”.

# ì½”ë“œ íŠ¸ë¦¬
ì½”ë“œì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  

![](assets/code_tree.png)
+ `utils.py`, `similarity.py`, `prompts.py`ë¥¼ ì»¤ìŠ¤í…€ ëª¨ë“ˆë¡œ `import`í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
+ ë°ì´í„° ìƒì„±ê³¼ í•™ìŠµ, í‰ê°€ëŠ” ì£¼í”¼í„° ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. 

[preprocess.ipynb](preprocess.ipynb)  
ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ë‹´ì•˜ìŠµë‹ˆë‹¤. ë°ì´í„° ìƒì„±ì€ gpt-4-turboì™€ gpt-4o APIë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.  
(ë°ì´í„°ë¥¼ ê°œì„ ì‹œí‚¤ëŠ” ë§‰ë°”ì§€ì— gpt-4oê°€ ì¶œì‹œë˜ì–´ ì›ê°€ ì ˆê°ì— í° ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ˜†)  
`preprocess.ipynb` ì½”ë“œëŠ” **ë¡œì»¬**ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.(Colabì—ì„œëŠ” ë³„ë„ì˜ ì„¤ì •ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼í•©ë‹ˆë‹¤.)  

[train.ipynb](train.ipynb)  
`Dataset` ìƒì„± ë“± ë°ì´í„° ì¤€ë¹„ê³¼ì •ê³¼ LoRAí•™ìŠµë“± Trainingì½”ë“œê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.  
`train.ipynb` ì½”ë“œëŠ” **Google Colab**ì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.  

[evaluation.ipynb](evaluation.ipynb)  
LLMìœ¼ë¡œ ë¶€í„° Evaluationì„ ë°›ëŠ” ì½”ë“œê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.
`evaluation.ipynb` ì½”ë“œëŠ” **Google Colab**ì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.  

# í›ˆë ¨ ìŠ¤í™
í•™ìŠµì€ ì‰½ê²Œ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆë„ë¡ [Google Colab](https://colab.google/)ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ì˜ˆì œ ëª¨ë¸ì˜ í›ˆë ¨ì‹œ ìŠ¤í™ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

| êµ¬ë¶„                          | ë‚´ìš©               |
|-----------------------------|------------------|
| í™˜ê²½                          | Google Colab     |
| GPU                         | L4(22.5GB)       |
| ì‚¬ìš© VRAM                     | ì•½ 13.8GB         |
| dtype                       | bfloat16         |
| Attention                   | flash attention2 |
| Tuning                      | Lora(r=4, alpha=32) |
| Learning Rate               | 1e-4             |
| LRScheduler                 | Cosine           |
| Optimizer                   | adamw_torch_fused |
| batch_size                  | 4                |
| gradient_accumulation_steps | 2                |

# Colab ê°€ì´ë“œ
Colabì—ì„œ ì‹¤í–‰í•˜ê¸°ìœ„í•´ [colab_guide.md](colab_guide.md)ë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”.

# ì°¸ì¡°
### ê¹ƒí—ˆë¸Œ ë° ê°€ì´ë“œ
+ [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
+ [chatgpt-prompt-engineering-for-developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers)
+ [awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)
+ [openai-python](https://github.com/openai/openai-python)
+ [Fine-Tuning Gemma Models in Hugging Face](https://huggingface.co/blog/gemma-peft)
+ [google/gemma-2b-it](https://huggingface.co/google/gemma-2b-it)
+ [Flash Attention](https://github.com/Dao-AILab/flash-attention)
+ [ë„¤ì´ë²„ API ê°€ì´ë“œ](https://developers.naver.com/docs/common/openapiguide/)
+ [transformers](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py)
+ [wandb](https://kr.wandb.ai/)
+ [Generative AI with LLMs](https://www.deeplearning.ai/courses/generative-ai-with-llms/)
+ [ì…ë¬¸ìë¥¼ ìœ„í•œ ë³‘ë ¬í”„ë¡œê·¸ë˜ë°](https://product.kyobobook.co.kr/detail/S000001875036)

### ë…¼ë¬¸
+ [Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/pdf/2403.08295)
+ [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560)
+ [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/pdf/2306.05685)
+ [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)


