{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L3YPLEIkHPB",
    "outputId": "5a855f5c-a0e1-49cb-d15f-12c9e7321520"
   },
   "source": [
    "### gemma-2b-it Instruction Tuning\n",
    "preprocess에서 전처리한 데이터를 이용해 gemma 모델을 학습시킵니다.\n",
    "코드들은 [Stanford alpaca](https://github.com/tatsu-lab/stanford_alpaca)를 기반으로 작성되었습니다.\n",
    "학습 환경은 아래와 같습니다.\n",
    "\n",
    "|구분|내용|\n",
    "|-|-|\n",
    "|학습환경|Google Colab|\n",
    "|GPU|L4(22.5GB)|\n",
    "|VRAM|약 17GB 소요|\n",
    "|dtype|bfloat16|\n",
    "|Attention|flash attention2|\n",
    "|Tuning|Lora(r=4, alpha=32)|\n",
    "|Learning Rate|1e-4|\n",
    "|Optimizer|adamw_torch_fused|\n",
    "\n",
    "---\n",
    "\n",
    "1. Pretrained 모델 설정  \n",
    "GPU 환경에서는 [flash attention](https://github.com/Dao-AILab/flash-attention)을 사용하는 것이 좋습니다.\n",
    "device는 cuda로 설정, dtype은 bfloat16으로 설정하였습니다.\n",
    "\n",
    "2. 토크나이저를 정의합니다.  \n",
    "추후 작성할 프롬프트들을 토크나이징하여 input_ids와 labels를 생성합니다.  \n",
    "\n",
    "3. 마스킹\n",
    "프롬프트들을 포맷팅한 다음 토크나이징 작업을 수행합니다.  \n",
    "학습에 사용할 데이터들을 마스킹 작업하여 훈련에 사용할 수 있게 합니다.\n",
    "\n",
    "4. `Dataset`과 `data_collator`를 정의합니다.  \n",
    "2번의 토크나이징한 데이터로 `Dataset`을 생성하여 `train_data_set`으로 설정합니다.\n",
    "`data_collator`를 통해 Sequence의 Padding을 수행합니다.\n",
    "\n",
    "5. Lora 설정\n",
    "Peft 학습을 위해 LoraConfig를 설정합니다.\n",
    "\n",
    "6. 학습\n",
    "1번의 Pretrained 모델, 4번의 Dataset 및 data_collator, 5번의 LoraConfig를 토대로 학습을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GfOPyFPRUYDV"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet\\\n",
    "selenium\\\n",
    "openai\\\n",
    "colorama\\\n",
    "datasets\\\n",
    "accelerate==0.27.2\\\n",
    "flash-attn\\\n",
    "peft\\\n",
    "trl\\\n",
    "transformers\\\n",
    "python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-29T08:49:02.165724Z",
     "iopub.status.busy": "2024-04-29T08:49:02.164966Z",
     "iopub.status.idle": "2024-04-29T08:49:06.960070Z",
     "shell.execute_reply": "2024-04-29T08:49:06.959824Z",
     "shell.execute_reply.started": "2024-04-29T08:49:02.165658Z"
    },
    "id": "UE8sMlakUPXa",
    "outputId": "294b60f2-cd23-4551-b759-e39fc112a88b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from google.colab import userdata, drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/instruction-tuning-with-rag-example-main\")\n",
    "import utils\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK20S6zuUPXb",
    "tags": []
   },
   "source": [
    "### 1. Pretrained 모델 설정\n",
    "[google/gemma-1.1-2b-it](https://huggingface.co/google/gemma-2b-it)  \n",
    "구글 Colab Secret 기능으로 허깅페이스 토큰을 사용할 수 있습니다.([Colab Secret 가이드](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75))  \n",
    "gemma 모델은 토큰이 없으면 받을 수 없습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습시 메모리 사용량을 크게 줄일 수 있는 방법은 3가지 정도였습니다.\n",
    "+ Quantization\n",
    "+ Flash Attention\n",
    "+ PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "`torch.bfloat16` 또는 `torch.float16` 타입으로 퍼포먼스를 비교적 떨어뜨리지 않으면서 적은 메모리를 사용할 수 있습니다.  \n",
    "온디바이스 AI가 아니라면 `int` 타입은 퍼포먼스 차이가 있으므로 유의해야합니다.  \n",
    "더 자세한 내용은 [huggingface](https://huggingface.co/docs/transformers/v4.16.2/en/performance#floating-data-types)를 참고하세요.  \n",
    "\n",
    "#### Flash Attention\n",
    "GPU를 사용하는 환경이면 flash attention을 사용하길 추천드립니다.  \n",
    "inference시 15GB정도의 VRAM이 소요되었지만, flash attention을 사용시 5GB 정도로 VRAM 사용량이 감소하였습니다.  \n",
    "flash attention은 속도와 메모리 사용량을 감소시킬 수 있지만 퍼포먼스의 큰 차이가 없습니다.\n",
    "자세한 내용은 [논문](https://arxiv.org/pdf/2307.08691https://arxiv.org/pdf/2307.08691)을 참조해주세요.\n",
    "\n",
    "#### PEFT\n",
    "가장 많이 사용되는 LoRA의 경우 논문에 따르면 GPU 사용량이 1/3 가량으로 절감되었다고 합니다.  \n",
    "또한 Full Fine Tuning과 비교하여 퍼포먼스의 차이도 크지 않다고 합니다(어떤 경우는 퍼포먼스가 더 좋았다고 합니다).\n",
    "자세한 내용은 [논문](https://arxiv.org/pdf/2106.09685)를 참조해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "24a13941ea824c0db0f9c922b7763ff5",
      "088488829c3d43b19b7b9b104c4cd3ba",
      "211a83dd7acb42a68d07b216b2cd11c1",
      "daea047196ee4e34bd8f056b6c0b6543",
      "fb76678a62854577b75e499a68c91172",
      "0b1ba26823494cf0b4c3e83241102120",
      "9aff4ba8256c4dc7a869ea8b3c889644",
      "125e0d37382a4aba8ba7eafbe03f232f",
      "12ebfcae9dce470fae7402ce5fee14a9",
      "2e58cc621b3a415db02c2d0f73acd39c",
      "54501be1ef114ec88ca7af37f8cf9eba"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:01.755641Z",
     "iopub.status.busy": "2024-04-29T08:50:01.755084Z",
     "iopub.status.idle": "2024-04-29T08:50:02.473996Z",
     "shell.execute_reply": "2024-04-29T08:50:02.473649Z",
     "shell.execute_reply.started": "2024-04-29T08:50:01.755606Z"
    },
    "id": "eMR9t93TGXgR",
    "outputId": "48a2faae-698f-4b75-e8b3-381c9105d20d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a13941ea824c0db0f9c922b7763ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "token = userdata.get('HF_TOKEN_READ') # Colab Secret에 설정 가능\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    "    token=token,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "# 토크나이저의 max_length가 매우 큰수로 지정되어있으므로, 모델의 sequence length인 8192로 세팅\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwGmTc37UPXb"
   },
   "source": [
    "### 2. tokenizer 설정\n",
    "프롬프트들을 받아 Embedding index를 반환하는 토크나이저 기능을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:16.509164Z",
     "iopub.status.busy": "2024-04-29T08:50:16.508842Z",
     "iopub.status.idle": "2024-04-29T08:50:16.514025Z",
     "shell.execute_reply": "2024-04-29T08:50:16.513353Z",
     "shell.execute_reply.started": "2024-04-29T08:50:16.509141Z"
    },
    "id": "Y1niFl07InYJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(strings, tokenizer):\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgRJcp8WUPXc"
   },
   "source": [
    "### 3. 마스킹\n",
    "데이터셋은 아래의 예시와 같이 전달됩니다.  \n",
    "+ `input_ids`\n",
    "```python\n",
    "tensor([  28693, 238264, 236648, 234541,  90621, 235248, 235284, 237029,  47555,\n",
    "        235265, 171066,  90621, 150483, 236800, 238597,  74715, 239618, 236137,\n",
    "          ...\n",
    "         95917,  96564,  75500, 237433, 235248, 235284, 237936, 237699,  31087,\n",
    "         96564,  83160, 237036, 149735, 179694, 235265,    108,      1])\n",
    "```\n",
    "\n",
    "+ `labels`\n",
    "```python\n",
    "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "          ...\n",
    "         95917,  96564,  75500, 237433, 235248, 235284, 237936, 237699,  31087,\n",
    "         96564,  83160, 237036, 149735, 179694, 235265,    108,      1])\n",
    "```\n",
    "\n",
    "Stanford Alpaca의 예시를 영문으로 변경하면 아래와 같은 형태입니다.  \n",
    "+ `input_ids`\n",
    "\n",
    "```plain\n",
    "</s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "Instruction:\n",
    "Give three tips for staying healthy.\n",
    "\n",
    "Response:\n",
    "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables.\n",
    "2. Exercise regularly to keep your body active and strong.\n",
    "3. Get enough sleep and maintain a consistent sleep schedule.</s>\n",
    "```\n",
    "\n",
    "\n",
    "+ `labels`\n",
    "\n",
    "```plain\n",
    "<MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK><MASK>\n",
    "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>\n",
    "```\n",
    "`<MASK>`는 제가 임의로 적어둔 것이며, 실제로는 -100 Index 값으로 채워지게 됩니다.  \n",
    "-100은 Pytorch의 모든 Loss 함수에서 `ignore_index`로 사용되는 값이므로, Loss 계산시 제외됩니다.  \n",
    "따라서 `input_ids`는 모든 Sequence, `labels`는 앞쪽은 마스킹되고 연산에 사용되는 Sequence는 ChatGPT의 답변 뿐입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:18.817416Z",
     "iopub.status.busy": "2024-04-29T08:50:18.816808Z",
     "iopub.status.idle": "2024-04-29T08:50:18.824863Z",
     "shell.execute_reply": "2024-04-29T08:50:18.823927Z",
     "shell.execute_reply.started": "2024-04-29T08:50:18.817385Z"
    },
    "id": "h2Ux4rAyI0Xl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 파이토치에서 Loss 함수들의 ignore_index 값들은 모두 -100으로 처리되어 있습니다\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    examples,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\" 프롬프트 데이터를 받아서 전처리\"\"\"\n",
    "    examples_tokenized, sources_tokenized = [tokenize(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3hNmPzWUPXc"
   },
   "source": [
    "### 4. `dataset`과 `collator` 정의\n",
    "+ 데이터셋을 로드하여 프롬프트에 포맷팅합니다.\n",
    "+ 포맷팅한 Sequence를 `Dataset` 객체로 정의합니다.\n",
    "+ batch 데이터를 처리하기 위해 `data_collator`를 정의합니다.\n",
    "+ 데이터셋과 `data_collator`를 `trainer`에 전달하기 쉽게 `dict` 객체로 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:20.554632Z",
     "iopub.status.busy": "2024-04-29T08:50:20.554094Z",
     "iopub.status.idle": "2024-04-29T08:50:20.561005Z",
     "shell.execute_reply": "2024-04-29T08:50:20.560379Z",
     "shell.execute_reply.started": "2024-04-29T08:50:20.554595Z"
    },
    "id": "J7ZuTMPV0tWm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(data_path: str):\n",
    "    \"\"\"데이터셋을 불러오고 프롬프트에 포맷팅합니다. 만든 데이터셋은 Dataset객체로 생성합니다.\"\"\"\n",
    "\n",
    "    instructions = utils.jload(data_path)\n",
    "    sources = []\n",
    "    examples = []\n",
    "    for idx in range(len(instructions)):\n",
    "        question = json.loads(instructions[idx])['question']\n",
    "        answer = json.loads(instructions[idx])['answer']\n",
    "        source = prompts.GEMMA_TRAINING_PROMPT.format(question=question, answer=\"\")\n",
    "        example = prompts.GEMMA_TRAINING_PROMPT.format(question=question, answer=answer) + tokenizer.eos_token\n",
    "        sources.append(source)\n",
    "        examples.append(example)\n",
    "\n",
    "    data_dict = preprocess(sources, examples, tokenizer)\n",
    "    train_dataset = Dataset.from_dict(data_dict)\n",
    "    train_dataset.set_format(\"torch\")\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:22.519030Z",
     "iopub.status.busy": "2024-04-29T08:50:22.518471Z",
     "iopub.status.idle": "2024-04-29T08:50:22.526870Z",
     "shell.execute_reply": "2024-04-29T08:50:22.525443Z",
     "shell.execute_reply.started": "2024-04-29T08:50:22.518991Z"
    },
    "id": "de1sseTKxfKZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"데이터 콜레이터\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids_ = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels_ = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids_,\n",
    "            labels=labels_,\n",
    "            attention_mask=input_ids_.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T08:50:23.266800Z",
     "iopub.status.busy": "2024-04-29T08:50:23.266241Z",
     "iopub.status.idle": "2024-04-29T08:50:23.272287Z",
     "shell.execute_reply": "2024-04-29T08:50:23.271342Z",
     "shell.execute_reply.started": "2024-04-29T08:50:23.266757Z"
    },
    "id": "tOKhFwQQLvvs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_supervised_data_module(data_path: str, tokenizer):\n",
    "    \"\"\"학습에 사용할 데이터셋과 콜레이터를 정의합니다.\"\"\"\n",
    "    train_dataset = make_train_dataset(data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXXaeU7EUPXd"
   },
   "source": [
    "### 5. Lora\n",
    "메모리 절감을 최대화 하기 위해 모델 내 Adapter를 추가할 수 있는 모든 Linear 연산에 추가합니다.\n",
    "+ Layer이름은 모델마다 다르니 모델의 Layer를 확인하고 추가해주세요.\n",
    "+ 모델의 Layer는 보통 `__repr__`로 정의되니 `model`만 입력해도 Layer가 출력됩니다.\n",
    "+ `TrainingArguments`의 경우, 여러번의 학습을 통해 적절한 하이퍼파라미터를 찾아서 학습시켰습니다.  \n",
    "\n",
    "아래 코드는 [huggingface의 블로그](https://huggingface.co/blog/gemma-pefthttps://huggingface.co/blog/gemma-peft)를 참조하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gm4g7RsvwgHn"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"gemma-2b-it-sgtuned\",\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RIyVfWMwxqLu"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.0,\n",
    "        r=4,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga6dh7RCUPXd"
   },
   "source": [
    "6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FiEcortZxX8K",
    "outputId": "9510181f-6c46-4da4-c6ec-5b86d5a5efb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/730 25:59, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.867700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.769300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.066200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.070500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f4df3-20ea1bbd20bfd3237fcf1a8b;2850963f-72b6-440e-9eb7-b4b0f540ed4c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f4e90-6352aece2ee46c465ee34749;95ac67da-3ea2-4f49-965e-1e008cd6f58c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f4f2d-4f8e3f4d21c3b309300608c9;24de25d2-cd71-421a-b81a-172e43c1183e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f4fcb-38c9023551e38c135271fbf8;acaafec7-d23c-461f-95b7-3f2ee1263d31)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5067-581a89e8698f757e06e95c3c;c7191e7a-faea-4b01-8857-8b5df3e5a8b3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5104-238e55590bb48db227435432;4bb84dd7-7d8d-480d-9f6d-541b65951b1f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f51a2-685147e070646ebd1d6b941e;ea24da01-ff8a-46c0-aab2-fc43aa83f21f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f523f-2d7f76c92f5c8ef35f9ee264;1066a8c7-b489-43dd-84ba-a4b0db8aa6a5)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f52dc-2cbd428b45b33ded2c428c6b;79bef588-d0c9-4cbc-9f78-a04206dea856)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f536f-3b865d977580033d682862cf;03ef39ef-d137-4d7c-a932-46545ebcef18)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-662f5370-756922b159faf97e75719501;18644a6f-7fa9-4d22-b63e-dd33b6820108)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-1.1-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-1.1-2b-it is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-1.1-2b-it.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-1.1-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "data_module = make_supervised_data_module(data_path=\"/content/drive/MyDrive/Colab Notebooks/gpt_with_tuning-main/tuning/instruction.jsonl\", tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    **data_module,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "64cedcc4b406462bbd12b60aece428de",
      "e5f17bc52fd7490c9d3a7e90199e0c36",
      "6c711628bf584c00ae2a35a0f7a8486d",
      "256553519e6045bea4e63858fbb6525d",
      "a373336b158b4c07abcf7dfbc6c19dee",
      "c3507755271b4684bb21ab7d67f27225",
      "20c2de90ee0646ec95541b1d5a1fd7cd",
      "69ca31099acb4a0d93f3e72370afc373",
      "c6b242a1b8c8496990ad72d72d8479cb",
      "90decf8eb8774633973b93acb69fb595",
      "af2a030de6d24d76b90f63b2e56d7695"
     ]
    },
    "id": "EGOw_Pm4W7We",
    "outputId": "f37c0cb7-a38e-4358-8436-e11188bb662a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cedcc4b406462bbd12b60aece428de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "my_model_id = \"aiqwe/gemma-2b-it-sgtuned\"\n",
    "my_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    my_model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "def generate(\n",
    "    model: transformers.PreTrainedModel,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    query: str,\n",
    "    rag: bool=False):\n",
    "\n",
    "    if rag:\n",
    "        documents = _scoring(query)\n",
    "        documents = \"\\n\".join(documents[:3])\n",
    "        query = query +\"\\n아래 documents를 참조하여 답변하세요\\n\" + documents\n",
    "\n",
    "    inputs = tokenizer.encode(query, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(input_ids=inputs, max_new_tokens=200)\n",
    "    completion = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        repetition_penalty = 1.5, # Greedy하게 답변하는것을 막기 위함\n",
    "        temperature = 0.5 # Stochastic한 답변 유도\n",
    "    )\n",
    "\n",
    "    return completion\n",
    "\n",
    "# gemma default template : https://huggingface.co/google/gemma-2b-it#chat-template\n",
    "GEMMA_TRAINING_PROMPT = \"\"\"<bos><start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOmyUx8-anMb",
    "outputId": "f803604d-6dd2-4d23-be23-312721603688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "청약에 대해 설명해줄래<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**청약**은 중국과 한국의 전통적인 민속 문화와 종교적 풍부한 의상입니다. 청약은 일반적으로 청소와 정화를 의미하며, 특히 청소와 정화를 통해 신체적, 정신적, 정제적 영향을 주는 것을 의미합니다.\n",
      "\n",
      "**청약의 역사:**\n",
      "\n",
      "* 청약의 역사는 2000년 이전까지 거슬러 올라갑니다.\n",
      "* 청약은 중국과 한국의 전통적인 민속 문화에 깊이 있게 연결되어 있습니다.\n",
      "* 청약은 또한 한국의 종교적 풍부한 역사와 연결되어 있습니다.\n",
      "\n",
      "**청약의 구성:**\n",
      "\n",
      "* 청약에는 일반적으로 다음과 같은 구성 요소가 포함됩니다.\n",
      "    * 청소\n",
      "    * 정화\n",
      "    * 수세\n",
      "    * 샤워\n",
      "    * 정\n"
     ]
    }
   ],
   "source": [
    "GEMMA_TRAINING_PROMPT = \"\"\"<bos><start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "query=\"청약에 대해 설명해줄래\"\n",
    "prompt = GEMMA_TRAINING_PROMPT.format(question=query)\n",
    "print(generate(model=model, tokenizer=tokenizer, query=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TGBdCxOWbCy",
    "outputId": "2c717f24-ad46-4107-f5e5-e9768441a81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "청약에 대해 설명해줄래<end_of_turn>\n",
      "<start_of_turn>model\n",
      "청약은 공공택지 내에서 주택을 건설하고 청약하는 방법입니다. 공공택지는 특정 조건을 충족하는 사람들에게 주택을 우선적으로 제공하는 곳입니다. 청약을 통해 주택을 받기 위해서는 특정 조건을 충족해야 하며, 일반적으로 먼저 무주택자나 실거주자이야 한 뒤에 일반인이 됩니다. 청약은 주택도시기본법에 따라 정해진 절차에 따라 진행되며, 청약통장을 가진 사람만이 청약을 할 수 있습니다. 청약통장은 특정 조건을 충족하는 사람에게 해당 공공택지 내에서 특정 기간 동안 주택을 청약할 수 있는 채권으로, 일반적으로 청약통장 1기, 2기, 3기가 있습니다. 각 채권에 따라\n"
     ]
    }
   ],
   "source": [
    "GEMMA_TRAINING_PROMPT = \"\"\"<bos><start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "query=\"청약에 대해 설명해줄래\"\n",
    "prompt = GEMMA_TRAINING_PROMPT.format(question=query)\n",
    "print(generate(model=my_model, tokenizer=tokenizer, query=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siIRFe5uW9fB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "088488829c3d43b19b7b9b104c4cd3ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b1ba26823494cf0b4c3e83241102120",
      "placeholder": "​",
      "style": "IPY_MODEL_9aff4ba8256c4dc7a869ea8b3c889644",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "0b1ba26823494cf0b4c3e83241102120": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "125e0d37382a4aba8ba7eafbe03f232f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12ebfcae9dce470fae7402ce5fee14a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20c2de90ee0646ec95541b1d5a1fd7cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "211a83dd7acb42a68d07b216b2cd11c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_125e0d37382a4aba8ba7eafbe03f232f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12ebfcae9dce470fae7402ce5fee14a9",
      "value": 2
     }
    },
    "24a13941ea824c0db0f9c922b7763ff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_088488829c3d43b19b7b9b104c4cd3ba",
       "IPY_MODEL_211a83dd7acb42a68d07b216b2cd11c1",
       "IPY_MODEL_daea047196ee4e34bd8f056b6c0b6543"
      ],
      "layout": "IPY_MODEL_fb76678a62854577b75e499a68c91172"
     }
    },
    "256553519e6045bea4e63858fbb6525d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90decf8eb8774633973b93acb69fb595",
      "placeholder": "​",
      "style": "IPY_MODEL_af2a030de6d24d76b90f63b2e56d7695",
      "value": " 2/2 [00:02&lt;00:00,  1.13it/s]"
     }
    },
    "2e58cc621b3a415db02c2d0f73acd39c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54501be1ef114ec88ca7af37f8cf9eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64cedcc4b406462bbd12b60aece428de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5f17bc52fd7490c9d3a7e90199e0c36",
       "IPY_MODEL_6c711628bf584c00ae2a35a0f7a8486d",
       "IPY_MODEL_256553519e6045bea4e63858fbb6525d"
      ],
      "layout": "IPY_MODEL_a373336b158b4c07abcf7dfbc6c19dee"
     }
    },
    "69ca31099acb4a0d93f3e72370afc373": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c711628bf584c00ae2a35a0f7a8486d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69ca31099acb4a0d93f3e72370afc373",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6b242a1b8c8496990ad72d72d8479cb",
      "value": 2
     }
    },
    "90decf8eb8774633973b93acb69fb595": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aff4ba8256c4dc7a869ea8b3c889644": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a373336b158b4c07abcf7dfbc6c19dee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af2a030de6d24d76b90f63b2e56d7695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3507755271b4684bb21ab7d67f27225": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6b242a1b8c8496990ad72d72d8479cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "daea047196ee4e34bd8f056b6c0b6543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e58cc621b3a415db02c2d0f73acd39c",
      "placeholder": "​",
      "style": "IPY_MODEL_54501be1ef114ec88ca7af37f8cf9eba",
      "value": " 2/2 [00:02&lt;00:00,  1.06it/s]"
     }
    },
    "e5f17bc52fd7490c9d3a7e90199e0c36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3507755271b4684bb21ab7d67f27225",
      "placeholder": "​",
      "style": "IPY_MODEL_20c2de90ee0646ec95541b1d5a1fd7cd",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "fb76678a62854577b75e499a68c91172": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
