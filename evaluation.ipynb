{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351fe4cf-3aa7-4203-afd0-e41a060f92d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "이 예제에서는 Evaluation을 해보겠습니다. `rouge`같은 벤치마크가 아닌 LLM을 이용한 평가 방법입니다.  \n",
    "물론 벤치마크 평가가 사장된 것은 아닙니다만, 현재는 LLM을 통한 평가 방법을 일반적으로 많이 사용하고 있습니다.  \n",
    "\n",
    "이 예제는 [https://arxiv.org/pdf/2306.05685](https://arxiv.org/pdf/2306.05685)와 [evals](https://github.com/openai/evals)바탕으로 작성되었습니다.  \n",
    "프롬프트 또한 논문을 기반으로 작성되었습니다.  \n",
    "\n",
    "평가는 아래의 순서로 진행됩니다.  \n",
    "1. 튜닝전 모델과 튜닝 후 모델을 불러옵니다.\n",
    "2. 작성해놓은 평가 프롬프트를 input으로 하여 각각의 모델로 output을 생성합니다.\n",
    "3. ChatGPT-4o 각 모델들의 output을 전달하여 평가하도록 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735fec2-f081-41ae-906f-9d49cb6cd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet\\\n",
    "accelerate\\\n",
    "flash-attn\\\n",
    "transformers\\\n",
    "openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0a5dc-3d7a-41b6-9940-03d8eb58312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from google.colab import userdata, drive\n",
    "# utils, prompts 커스텀 모듈 사용을 위해 구글 드라이브에 마운트합니다.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# git clone 받은 위치로 지정합니다.\n",
    "path = \"/content/drive/MyDrive/instruction-tuning-with-rag-example\"\n",
    "\n",
    "sys.path.append(path)\n",
    "import utils\n",
    "import prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a1538-fbf5-4a33-ae95-84ed1141694a",
   "metadata": {},
   "source": [
    "### 1. 튜닝전 모델과 튜닝 후 모델을 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd526cc6-f826-45d5-96c9-0922f6620eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=userdata.get(\"HF_TOKEN\"),\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.path.join(path, \"gemma-2b-it-example-v1\"),\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c7994-25aa-473a-8f3d-4ad757fe7601",
   "metadata": {},
   "source": [
    "### 2. 작성해놓은 평가 프롬프트를 input으로 하여 각각의 모델로 output을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10787116-e222-4380-9ed0-c406d08df4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터 불러오기\n",
    "with open(os.path.join(path, \"data/eval_dataset.txt\"), \"r\") as f:\n",
    "    eval_inputs = f.readlines()\n",
    "\n",
    "eval_inputs = [inputs.strip(\"\\n\") for inputs in eval_inputs]\n",
    "eval_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69b218-0ad8-4397-bec8-b0bde169ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate\n",
    "inputs = eval_inputs\n",
    "inputs_with_tag = [instruction + \"<ANSWER>\" for instruction in inputs]\n",
    "\n",
    "input_text = tokenizer(inputs_with_tag, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "base_outputs = model.generate(**input_text, max_new_tokens=512, repetition_penalty = 1.5)\n",
    "base_decoded = tokenizer.batch_decode(base_outputs, skip_special_tokens=True)\n",
    "base_decoded = [decoded.split(\"<ANSWER>\")[1] for decoded in base_decoded]\n",
    "\n",
    "finetuned_outputs = finetuned_model.generate(**input_text, max_new_tokens=512, repetition_penalty = 1.5)\n",
    "finetuned_decoded = tokenizer.batch_decode(finetuned_outputs, skip_special_tokens=True)\n",
    "finetuned_decoded = [decoded.split(\"<ANSWER>\")[1] for decoded in finetuned_decoded]\n",
    "\n",
    "eval_dataset = dict(inputs=inputs, completion1=base_decoded, completion2=finetuned_decoded)\n",
    "utils.jsave(data=eval_dataset, save_path=os.path.join(path, \"data/eval_dataset.json\"), mode=\"w\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da43ab-1f28-4db7-975b-8cde0f163814",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = utils.jload(\"./data/eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be043d-7c2b-4206-a075-6dc7db824669",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i1, c1, i2, c2 in zip(eval_dataset['input1'], eval_dataset['completion1'], eval_dataset['input2'], eval_dataset['completion2']):\n",
    "    utils.jsave(dict(input1=i1, completion1=c1, input2=i2, completion2=c2), \"./data/eval_dataset.jsonl\", \"a\")\n",
    "    with open(\"./data/eval_dataset.jsonl\", \"a\") as f:\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d4bc7-ceaf-49c2-80d2-a870b379b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = utils.jload(\"./data/eval_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b53f2-5476-4501-85b2-8b40ed318269",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for evals in eval_dataset:\n",
    "    data = json.loads(evals)\n",
    "    res = utils.get_completion(prompts.EVAL_BATTLE_PROMPT.format(question=data['input1'], answer_a=data['completion1'], answer_b=data['completion2']))\n",
    "    scores.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4b61c-7946-44e4-8d3d-bdc6a625faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for evals in eval_dataset:\n",
    "    data = json.loads(evals)\n",
    "    res = utils.get_completion(prompts.EVAL_SCORE_PROMPT.format(question=data['input1'], answer_a=data['completion1'], answer_b=data['completion2']))\n",
    "    scores.append(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
